# === Importation des librairies n√©cessaires ===
import pandas as pd
import numpy as np
import warnings
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")  # Ignorer les avertissements pour plus de clart√©
# === Chargement des donn√©es ===
train = pd.read_csv("/Users/andalouse/data/train.csv")
test = pd.read_csv("/Users/andalouse/data/test.csv")
census = pd.read_csv("/Users/andalouse/data/census_starter.csv")
revealed = pd.read_csv("/Users/andalouse/data/revealed_test.csv")

# === Garder seulement les colonnes pertinentes ===
train = train[["row_id", "cfips", "first_day_of_month", "microbusiness_density"]]  # Garde les colonnes utiles
train = train.sort_values(["cfips", "first_day_of_month"])  # Trie les donn√©es par comt√© et par date

# === Calcul du taux de croissance (variable cible) ===
train["growth_rate"] = train.groupby("cfips")["microbusiness_density"].pct_change()  # Calcul du pourcentage de changement mois √† mois
train["growth_rate"] = train["growth_rate"].replace([np.inf, -np.inf], np.nan)        # Supprime les infinis √©ventuels
train["growth_rate"] = train["growth_rate"].clip(-0.8, 0.8)                           # Limite les valeurs extr√™mes
train = train.dropna(subset=["growth_rate"])                                          # Supprime les lignes sans cible

# === Cr√©ation des variables de d√©calage (lags) ===
for lag in [1, 2, 3]:
    train[f"mbd_lag_{lag}"] = train.groupby("cfips")["microbusiness_density"].shift(lag)  # Valeur des 1, 2, 3 mois pr√©c√©dents

# === Cr√©ation des moyennes glissantes ===
for window in [3, 6]:
    train[f"roll_mean_{window}"] = (
        train.groupby("cfips")["microbusiness_density"]
        .transform(lambda x: x.shift(1).rolling(window).mean())  # Moyenne des valeurs pr√©c√©dentes sur 3 et 6 mois
    )

# === Extraction de la date en variables ann√©e et mois ===
train["first_day_of_month"] = pd.to_datetime(train["first_day_of_month"])  # Conversion en format date
train["year"] = train["first_day_of_month"].dt.year   # Extraction de l‚Äôann√©e
train["month"] = train["first_day_of_month"].dt.month # Extraction du mois

# === Pr√©paration des variables de recensement (census) ===
census = census.rename(columns={  # Renomme les colonnes de 2020 pour simplifier les noms
    "median_hh_inc_2020": "median_hh_inc",
    "pct_bb_2020": "pct_bb",
    "pct_college_2020": "pct_college",
    "pct_foreign_born_2020": "pct_foreign_born",
    "pct_it_workers_2020": "pct_it_workers"
})
cols_to_keep = ["cfips", "median_hh_inc", "pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers"]  # Liste des colonnes utiles
train = train.merge(census[cols_to_keep], on="cfips", how="left")  # Fusion des donn√©es socio-√©conomiques sur le code du comt√©

# === D√©coupage temporel du jeu de donn√©es (train / validation) ===
cutoff = pd.Timestamp("2022-08-01")  # Date de coupure temporelle
train_df = train[train["first_day_of_month"] <= cutoff]  # Donn√©es d'entra√Ænement (avant ao√ªt 2022)
val_df = train[train["first_day_of_month"] > cutoff]     # Donn√©es de validation (apr√®s ao√ªt 2022)

# === D√©finition de la liste des variables explicatives (features) ===
features = [
    "cfips", "month", "year",
    "mbd_lag_1", "mbd_lag_2", "mbd_lag_3",
    "roll_mean_3", "roll_mean_6",
    "median_hh_inc", "pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers"
]

# === D√©finition de la fonction de m√©trique SMAPE ===
def smape(y_true, y_pred):
    num = np.abs(y_true - y_pred)                           # Valeurs absolues des erreurs
    den = (np.abs(y_true) + np.abs(y_pred)) / 2             # Moyenne des valeurs vraies et pr√©dites
    mask = (y_true != 0) | (y_pred != 0)                    # √âvite la division par z√©ro
    return 100 * np.mean(num[mask] / den[mask])             # Retourne le SMAPE en pourcentage


# === Entra√Ænement du mod√®le LightGBM ===
# === Entra√Ænement du mod√®le LightGBM ===
model = LGBMRegressor(               # Initialisation du mod√®le LightGBM
    objective="regression_l1",       # Fonction de perte robuste aux valeurs extr√™mes (L1)
    learning_rate=0.05,              # Taux d‚Äôapprentissage
    n_estimators=700,                # Nombre total d‚Äôarbres
    num_leaves=64,                   # Complexit√© des arbres (nombre de feuilles)
    subsample=0.9,                   # Proportion d‚Äô√©chantillons utilis√©s par arbre
    colsample_bytree=0.8,            # Proportion de variables utilis√©es par arbre
    random_state=42,                 # Graine al√©atoire pour reproductibilit√©
)

model.fit(train_df[features], train_df["growth_rate"])  # Entra√Æne le mod√®le sur les variables d‚Äôentr√©e et la cible

# === Validation du mod√®le ===
val_pred_growth = model.predict(val_df[features])            # Pr√©dictions du taux de croissance sur le jeu de validation
val_pred_density = (1 + val_pred_growth) * val_df["mbd_lag_1"]  # Conversion du taux de croissance en densit√© r√©elle
smape_val = smape(val_df["microbusiness_density"], val_pred_density)  # Calcul du SMAPE sur la validation
mae_val = mean_absolute_error(val_df["microbusiness_density"], val_pred_density)  # Calcul du MAE
# Les deux m√©triques √©valuent la pr√©cision du mod√®le

# === Pr√©paration des donn√©es de test ===
test["first_day_of_month"] = pd.to_datetime(test["first_day_of_month"])  # Conversion du champ date
test["year"] = test["first_day_of_month"].dt.year                        # Extraction de l‚Äôann√©e
test["month"] = test["first_day_of_month"].dt.month                      # Extraction du mois

# === Initialisation de l‚Äôhistorique pour les pr√©visions r√©cursives ===
hist = train.groupby("cfips").tail(6)[["cfips", "first_day_of_month", "microbusiness_density"]].copy()  # Six derniers mois par comt√©
hist = hist.sort_values(["cfips", "first_day_of_month"])  # Trie chronologiquement l‚Äôhistorique
results = []  # Liste pour stocker les r√©sultats de pr√©diction

# === Boucle de pr√©vision r√©cursive pour chaque mois futur ===
for date in sorted(test["first_day_of_month"].unique()):  # Parcourt chaque mois √† pr√©dire
    print(f"‚è© Predicting {date.date()}")

    temp = hist.groupby("cfips").tail(6).copy()  # R√©cup√®re les six derni√®res observations par comt√©

    # Cr√©ation des variables de d√©calage (lags)
    for lag in [1, 2, 3]:
        temp[f"mbd_lag_{lag}"] = temp.groupby("cfips")["microbusiness_density"].shift(lag)

    # Cr√©ation des moyennes mobiles
    for window in [3, 6]:
        temp[f"roll_mean_{window}"] = temp.groupby("cfips")["microbusiness_density"].transform(
            lambda x: x.shift(1).rolling(window).mean()
        )

    # Garde la derni√®re ligne compl√®te par comt√©
    latest = temp.dropna(subset=["mbd_lag_1"]).drop_duplicates(subset=["cfips"], keep="last")
    latest = latest[["cfips", "mbd_lag_1", "mbd_lag_2", "mbd_lag_3", "roll_mean_3", "roll_mean_6"]]

    # Fusionne avec les donn√©es de test et de recensement
    step_df = test[test["first_day_of_month"] == date].merge(latest, on="cfips", how="left")
    step_df = step_df.merge(census[cols_to_keep], on="cfips", how="left")
    step_df[features] = step_df[features].ffill().bfill()  # Remplit les valeurs manquantes par propagation

    # Pr√©diction du mod√®le sur le mois courant
    growth_pred = model.predict(step_df[features])          # Pr√©diction du taux de croissance
    step_df["pred_density"] = (1 + growth_pred) * step_df["mbd_lag_1"]  # Conversion en densit√© pr√©dite

    # Mise √† jour de l‚Äôhistorique avec les nouvelles pr√©dictions
    hist = pd.concat([
        hist,
        step_df[["cfips", "first_day_of_month", "pred_density"]]
        .rename(columns={"pred_density": "microbusiness_density"})
    ])

    results.append(step_df[["row_id", "pred_density"]])  # Sauvegarde des pr√©dictions du mois

# === Cr√©ation du fichier final de soumission ===
submission = pd.concat(results).rename(columns={"pred_density": "microbusiness_density"})  # Concat√®ne tous les mois
submission.to_csv("submission.csv", index=False)  # Sauvegarde le fichier CSV


print("‚úÖ Forecast complete. Saved as submission.csv.")
print(submission.head())

# === √âvaluation finale avec le jeu de test r√©v√©l√© ===
merged = submission.merge(  # Fusion des pr√©dictions et des vraies valeurs du jeu r√©v√©l√©
    revealed[["row_id", "microbusiness_density"]],
    on="row_id", how="inner", suffixes=("_pred", "_true")
)

mae = np.mean(np.abs(merged.microbusiness_density_pred - merged.microbusiness_density_true))  # Calcul du MAE global
smape_val = smape(merged.microbusiness_density_true, merged.microbusiness_density_pred)       # Calcul du SMAPE global

# === Section de visualisation pour le rapport ===
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Style g√©n√©ral pour des graphiques homog√®nes et lisibles
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_context("talk", font_scale=1)
sns.set_palette("muted")

# === 1Ô∏è‚É£ Graphique temporel d‚Äôun comt√© (Time-Series Forecast) ===
cfips_example = 1001  # Exemple : comt√© 1001
train_cfips = train[train.cfips == cfips_example].copy()  # Donn√©es d‚Äôentra√Ænement pour ce comt√©
pred_cfips = submission[submission.row_id.str.startswith(str(cfips_example))].copy()  # Pr√©dictions correspondantes
pred_cfips["first_day_of_month"] = pd.to_datetime(pred_cfips["row_id"].str[-10:])  # Extraction de la date depuis row_id

# Combine les donn√©es historiques et pr√©dites pour un affichage continu
all_cfips = pd.concat([
    train_cfips[["first_day_of_month", "microbusiness_density"]].assign(Source="Train"),
    pred_cfips[["first_day_of_month", "microbusiness_density"]].assign(Source="Forecast")
])

plt.figure(figsize=(11, 6))
sns.lineplot(  # Courbes liss√©es pour visualiser l‚Äô√©volution temporelle
    data=all_cfips,
    x="first_day_of_month",
    y="microbusiness_density",
    hue="Source",
    style="Source",
    markers=True,
    dashes=False,
    linewidth=2.5,
    palette={"Train": "#1f77b4", "Forecast": "#ff7f0e"},
)

# Zone ombr√©e indiquant la p√©riode de pr√©vision
forecast_start = pred_cfips["first_day_of_month"].min()
plt.axvspan(forecast_start, all_cfips["first_day_of_month"].max(),
            color="#ff7f0e", alpha=0.08, label="P√©riode de pr√©vision")

plt.title(f"üìà Pr√©vision de la densit√© des microentreprises ‚Äî comt√© {cfips_example}",
          fontsize=17, fontweight="bold", pad=15)
plt.xlabel("Date", fontsize=13)
plt.ylabel("Densit√© des microentreprises", fontsize=13)
plt.legend(frameon=True, loc="upper left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()


# === 2Ô∏è‚É£ Graphique d‚Äôimportance des variables (Feature Importance) ===
imp = pd.DataFrame({  # Cr√©e une table des importances
    "Feature": features,
    "Importance": model.feature_importances_
}).sort_values("Importance", ascending=True)  # Trie les variables de la moins √† la plus importante

plt.figure(figsize=(8, 6))
sns.barplot(  # Graphique en barres horizontales
    data=imp,
    y="Feature",
    x="Importance",
    palette="Blues_d"
)
plt.title("üîç Importance des variables (Mod√®le LightGBM)",
          fontsize=17, fontweight="bold", pad=12)
plt.xlabel("Score d‚Äôimportance", fontsize=13)
plt.ylabel("")
plt.tight_layout()
plt.show()


# === 3Ô∏è‚É£ Nuage de points de validation (Predicted vs True) ===
val_results = pd.DataFrame({  # Assemble les valeurs r√©elles et pr√©dites
    "True": val_df["microbusiness_density"],
    "Predicted": val_pred_density
})

plt.figure(figsize=(7, 7))
sns.scatterplot(  # Repr√©sente la corr√©lation entre valeurs r√©elles et pr√©dites
    data=val_results,
    x="True",
    y="Predicted",
    alpha=0.6,
    s=50,
    color="#4C72B0",
    edgecolor="white",
    linewidth=0.5
)

# Ajout de la ligne rouge id√©ale (y = x)
max_val = max(val_results.max())
plt.plot([0, max_val], [0, max_val], "r--", lw=2.5, label="Pr√©diction parfaite")

plt.title("üéØ Validation ‚Äî Valeurs pr√©dites vs r√©elles",
          fontsize=17, fontweight="bold", pad=15)
plt.xlabel("Valeurs r√©elles", fontsize=13)
plt.ylabel("Valeurs pr√©dites", fontsize=13)
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()
